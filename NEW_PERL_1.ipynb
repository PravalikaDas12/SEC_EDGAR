{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3ea74dc0",
   "metadata": {},
   "source": [
    "Note: Please change the directories according to the ones in which you store the data. I have tried commenting and explaining the codes wherever possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b5fa6",
   "metadata": {},
   "source": [
    "I have tried creating new files for every output that will help us in tracking the effeiciency of the codes. Till now, most of the codes work quite fine. However, it takes a long time for the codes to work on the files\n",
    "\n",
    "The computer might also give warnings of High Memory Alert. One can simply ignore them and continue with their execution of the codes. The codes are written with reference from the Perl Script given in the Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c47f06",
   "metadata": {},
   "source": [
    "# Description: These 2 following codes shows how to transform the raw data to a useable format. I have added 2 different codes. In case one doesn't works, one can try using the 2nd one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def clean_and_prepare_10k(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Remove header and irrelevant sections\n",
    "        cleaned_text = re.sub(r'UNITED STATES SECURITIES AND EXCHANGE COMMISSION[\\s\\S]+?(?=EXPLANATORY NOTE)', '', text)\n",
    "\n",
    "        # Extract relevant sections\n",
    "        sections = re.findall(r'(ITEM \\d+\\..*?)(?=ITEM \\d+\\.|\\Z)', cleaned_text, re.DOTALL)\n",
    "\n",
    "        # Format sections\n",
    "        formatted_sections = []\n",
    "        for section in sections:\n",
    "            # Remove extra whitespace and newlines\n",
    "            section = section.strip()\n",
    "            # Remove section indicators like 'ITEM X.'\n",
    "            section = re.sub(r'ITEM \\d+\\.', '', section)\n",
    "            formatted_sections.append(section)\n",
    "\n",
    "    return formatted_sections\n",
    "\n",
    "def save_formatted_sections(file_path, sections):\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    new_file_name = os.path.splitext(file_name)[0] + '_formatted.txt'\n",
    "    new_file_path = os.path.join(file_dir, new_file_name)\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "        for i, section in enumerate(sections, 1):\n",
    "            new_file.write(f\"Section {i}:\\n{section}\\n{'-'*50}\\n\")\n",
    "    print(f\"Formatted sections saved to: {new_file_path}\")\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'naya/naya2/naya3/target.txt'\n",
    "sections = clean_and_prepare_10k(file_path)\n",
    "save_formatted_sections(file_path, sections)\n",
    "\n",
    "# Print the formatted sections\n",
    "for i, section in enumerate(sections, 1):\n",
    "    print(f\"Section {i}:\\n{section}\\n{'-'*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "def trim(string):\n",
    "    return string.strip()\n",
    "\n",
    "def trimend(string):\n",
    "    return string.rstrip() + '\\n'\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.is_html = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag.lower() == 'html':\n",
    "            self.is_html = True\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        self.data.append(data)\n",
    "\n",
    "    def get_text(self):\n",
    "        return ''.join(self.data)\n",
    "\n",
    "def alter_html_code_to_character(line):\n",
    "    # Alter encoded HTML characters to convert to proper characterset.\n",
    "    html_codes = {\n",
    "        '&#32;': ' ', '&#33;': '!', '&#34;': '\"', '&#35;': '#', '&#36;': '$', '&#37;': '%', '&#38;': '&', '&#39;': \"'\",\n",
    "        '&#40;': '(', '&#41;': ')', '&#42;': '*', '&#43;': '+', '&#44;': ',', '&#45;': '-', '&#46;': '.', '&#47;': '/',\n",
    "        '&#48;': '0', '&#49;': '1', '&#50;': '2', '&#51;': '3', '&#52;': '4', '&#53;': '5', '&#54;': '6', '&#55;': '7',\n",
    "        '&#56;': '8', '&#57;': '9', '&#58;': ':', '&#59;': ';', '&#60;': '<', '&#61;': '=', '&#62;': '>', '&#63;': '?',\n",
    "        '&#64;': '@', '&#65;': 'A', '&#66;': 'B', '&#67;': 'C', '&#68;': 'D', '&#69;': 'E', '&#70;': 'F', '&#71;': 'G',\n",
    "        '&#72;': 'H', '&#73;': 'I', '&#74;': 'J', '&#75;': 'K', '&#76;': 'L', '&#77;': 'M', '&#78;': 'N', '&#79;': 'P',\n",
    "        '&#80;': 'P', '&#81;': 'Q', '&#82;': 'R', '&#83;': 'S', '&#84;': 'T', '&#85;': 'U', '&#86;': 'V', '&#87;': 'W',\n",
    "        '&#88;': 'X', '&#89;': 'Y', '&#90;': 'Z', '&#91;': '[', '&#92;': '\\\\', '&#93;': ']', '&#94;': '^', '&#95;': '_',\n",
    "        '&#96;': '`', '&#97;': 'a', '&#98;': 'b', '&#99;': 'c', '&#100;': 'd', '&#101;': 'e', '&#102;': 'f', '&#103;': 'g',\n",
    "        '&#104;': 'h', '&#105;': 'i', '&#106;': 'j', '&#107;': 'k', '&#108;': 'l', '&#109;': 'm', '&#110;': 'n', '&#111;': 'o',\n",
    "        '&#112;': 'p', '&#113;': 'q', '&#114;': 'r', '&#115;': 's', '&#116;': 't', '&#117;': 'u', '&#118;': 'v', '&#119;': 'w',\n",
    "        '&#120;': 'x', '&#121;': 'y', '&#122;': 'z', '&#123;': '{', '&#124;': '|', '&#125;': '}', '&#126;': '~', '&#145;': \"'\",\n",
    "        '&#146;': \"'\", '&#147;': '\"', '&#148;': '\"', '&#160;': ' ', '&#161;': '¡', '&#162;': '¢', '&#163;': '£', '&#164;': '¤',\n",
    "        '&#165;': '¥', '&#166;': '¦', '&#167;': '§', '&#168;': '¨', '&#169;': '©', '&#170;': 'ª', '&#171;': '«', '&#172;': '¬',\n",
    "        '&#174;': '®', '&#175;': '¯', '&#176;': '°', '&#177;': '±', '&#178;': '²', '&#179;': '³', '&#180;': '´', '&#181;': 'µ',\n",
    "        '&#182;': '¶', '&#183;': '·', '&#184;': '¸', '&#185;': '¹', '&#186;': 'º', '&#187;': '»', '&#188;': '¼', '&#189;': '½',\n",
    "        '&#190;': '¾', '&#191': '¿', '&#8211;': '–', '&#8212;': '—', '&#8216;': '‘', '&#8217;': '’', '&#8218;': '‚', '&#8220;': '\"',\n",
    "        '&#8221;': '\"', '&#8364;': '€', '&#8482;': '™', '&ndash;': '–', '&mdash;': '—', '&iexcl;': '¡', '&iquest;': '¿',\n",
    "        '&quot;': '\"', '&laquo;': '«', '&raquo;': '»', '&nbsp;': ' ', '&amp;': '&', '&cent;': '¢', '&copy;': '©', '&divide;': '÷',\n",
    "        '&gt;': '>', '&lt;': '<', '&middot;': '·', '&para;': '¶', '&plusmn;': '±', '&euro;': '€', '&pound;': '£', '&reg;': '®',\n",
    "        '&sect;': '§', '&yen;': '¥', '&aacute;': 'á', '&Aacute;': 'Á', '&agrave;': 'à', '&Agrave;': 'À', '&acirc;': 'â',\n",
    "        '&Acirc;': 'Â', '&aring;': 'å', '&Aring;': 'Å', '&atilde;': 'ã', '&Atilde;': 'Ã', '&auml;': 'ä', '&Auml;': 'Ä',\n",
    "        '&aelig;': 'æ', '&AElig;': 'Æ', '&ccedil;': 'ç', '&Ccedil;': 'Ç', '&eacute;': 'é', '&Eacute;': 'É', '&egrave;': 'è',\n",
    "        '&Egrave;': 'È', '&ecirc;': 'ê', '&Ecirc;': 'Ê', '&euml;': 'ë', '&Euml;': 'Ë', '&iacute;': 'í', '&Iacute;': 'Í',\n",
    "        '&igrave;': 'ì', '&Igrave;': 'Ì', '&icirc;': 'î', '&Icirc;': 'Î', '&iuml;': 'ï', '&Iuml;': 'Ï', '&ntilde;': 'ñ',\n",
    "        '&Ntilde;': 'Ñ', '&oacute;': 'ó', '&Oacute;': 'Ó', '&ograve;': 'ò', '&Ograve;': 'Ò', '&ocirc;': 'ô', '&Ocirc;': 'Ô',\n",
    "        '&oslash;': 'ø', '&Oslash;': 'Ø', '&otilde;': 'õ', '&Otilde;': 'Õ', '&ouml;': 'ö', '&Ouml;': 'Ö', '&szlig;': 'ß',\n",
    "        '&uacute;': 'ú', '&Uacute;': 'Ú', '&ugrave;': 'ù', '&Ugrave;': 'Ù', '&ucirc;': 'û', '&Ucirc;': 'Û', '&uuml;': 'ü',\n",
    "        '&Uuml;': 'Ü', '&yuml;': 'ÿ', '&quot;': '\"', '&apos;': \"'\", '&amp;': '&', '&lt;': '<', '&gt;': '>', '&nbsp;': ' ',\n",
    "        '&iexcl;': '¡', '&cent;': '¢', '&pound;': '£', '&curren;': '¤', '&yen;': '¥', '&brvbar;': '¦', '&sect;': '§',\n",
    "        '&uml;': '¨', '&copy;': '©', '&ordf;': 'ª', '&laquo;': '«', '&not;': '¬', '&shy;': '­', '&reg;': '®', '&macr;': '¯',\n",
    "        '&deg;': '°', '&plusmn;': '±', '&sup2;': '²', '&sup3;': '³', '&acute;': '´', '&micro;': 'µ', '&para;': '¶',\n",
    "        '&middot;': '·', '&cedil;': '¸', '&sup1;': '¹', '&ordm;': 'º', '&raquo;': '»', '&frac14;': '¼', '&frac12;': '½',\n",
    "        '&frac34;': '¾', '&iquest;': '¿', '&times;': '×', '&divide;': '÷', '&Agrave;': 'À', '&Aacute;': 'Á', '&Acirc;': 'Â',\n",
    "        '&Atilde;': 'Ã', '&Auml;': 'Ä', '&Aring;': 'Å', '&AElig;': 'Æ', '&Ccedil;': 'Ç', '&Egrave;': 'È', '&Eacute;': 'É',\n",
    "        '&Ecirc;': 'Ê', '&Euml;': 'Ë', '&Igrave;': 'Ì', '&Iacute;': 'Í', '&Icirc;': 'Î', '&Iuml;': 'Ï', '&ETH;': 'Ð',\n",
    "        '&Ntilde;': 'Ñ', '&Ograve;': 'Ò', '&Oacute;': 'Ó', '&Ocirc;': 'Ô', '&Otilde;': 'Õ', '&Ouml;': 'Ö', '&times;': '×',\n",
    "        '&Oslash;': 'Ø', '&Ugrave;': 'Ù', '&Uacute;': 'Ú', '&Ucirc;': 'Û', '&Uuml;': 'Ü', '&Yacute;': 'Ý', '&THORN;': 'Þ',\n",
    "        '&szlig;': 'ß', '&agrave;': 'à', '&aacute;': 'á', '&acirc;': 'â', '&atilde;': 'ã', '&auml;': 'ä', '&aring;': 'å',\n",
    "        '&aelig;': 'æ', '&ccedil;': 'ç', '&egrave;': 'è', '&eacute;': 'é', '&ecirc;': 'ê', '&euml;': 'ë', '&igrave;': 'ì',\n",
    "        '&iacute;': 'í', '&icirc;': 'î', '&iuml;': 'ï', '&eth;': 'ð', '&ntilde;': 'ñ', '&ograve;': 'ò', '&oacute;': 'ó',\n",
    "        '&ocirc;': 'ô', '&otilde;': 'õ', '&ouml;': 'ö', '&divide;': '÷', '&oslash;': 'ø', '&ugrave;': 'ù', '&uacute;': 'ú',\n",
    "        '&ucirc;': 'û', '&uuml;': 'ü', '&yacute;': 'ý', '&thorn;': 'þ', '&yuml;': 'ÿ', '&quot;': '\"', '&amp;': '&',\n",
    "        '&apos;': \"'\", '&lt;': '<', '&gt;': '>', '&nbsp;': ' ', '&iexcl;': '¡', '&cent;': '¢', '&pound;': '£',\n",
    "        '&curren;': '¤', '&yen;': '¥', '&brvbar;': '¦', '&sect;': '§', '&uml;': '¨', '&copy;': '©', '&ordf;': 'ª',\n",
    "        '&laquo;': '«', '&not;': '¬', '&shy;': '­', '&reg;': '®', '&macr;': '¯', '&deg;': '°', '&plusmn;': '±',\n",
    "        '&sup2;': '²', '&sup3;': '³', '&acute;': '´', '&micro;': 'µ', '&para;': '¶', '&middot;': '·', '&cedil;': '¸',\n",
    "        '&sup1;': '¹', '&ordm;': 'º', '&raquo;': '»', '&frac14;': '¼', '&frac12;': '½', '&frac34;': '¾', '&iquest;': '¿',\n",
    "        '&times;': '×', '&divide;': '÷'\n",
    "    }\n",
    "\n",
    "    for code, char in html_codes.items():\n",
    "        line = line.replace(code, char)\n",
    "        print(\"done\")\n",
    "    return line\n",
    "\n",
    "def clean_html_file(source_html, target_text):\n",
    "    parser = MyHTMLParser()\n",
    "    with open(source_html, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            parser.feed(line)\n",
    "            text = parser.get_text()\n",
    "            text = trim(text)\n",
    "            text = trimend(text)\n",
    "            \n",
    "            if text:\n",
    "                print(text,\"\\n\")\n",
    "                try:\n",
    "                    with open(target_text, 'w', encoding='utf-8') as out_file:\n",
    "                        out_file.write(text)\n",
    "                        out_file.write('\\n')\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while appending to the file: {e}\")  \n",
    "                    \n",
    "source_file = \"C:\\\\Users\\\\Pravalika Das\\\\DATA_WALA\\\\UPDATED_NLP_COURSE\\\\SEC EDGAR\\\\naya\\\\naya2\\\\naya3\\\\SEC.txt\"\n",
    "target_file = \"C:\\\\Users\\\\Pravalika Das\\\\DATA_WALA\\\\UPDATED_NLP_COURSE\\\\SEC EDGAR\\\\naya\\\\naya2\\\\naya3\\\\target2.txt\"\n",
    "\n",
    "clean_html_file(source_file, target_file)\n",
    "\n",
    "# You might want to add more code here to verify if the cleaning process was successful or to handle exceptions.\n",
    "print(\"HTML file cleaned and saved to text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee93556",
   "metadata": {},
   "source": [
    "# Description: This script finds and extracts the footnotes section from the cleaned 10-K filings text files;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5727c",
   "metadata": {},
   "source": [
    "This code displays the raw data that was converted to the formatted data to a proper footnotes kind of file with extra spaces, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69483312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Define regular expressions for header information\n",
    "header_formtype = re.compile(r'^\\s*FORM\\s*TYPE:\\s*(.*)')\n",
    "header_cik = re.compile(r'CENTRAL INDEX KEY:\\s{1,}([0-9]{10})\\s{0,}')\n",
    "header_filedate = re.compile(r'FILED AS OF DATE:\\s{1,}([0-9]{8})\\s{0,}')\n",
    "header_fiscdate = re.compile(r'CONFORMED PERIOD OF REPORT:\\s{1,}([0-9]{8})\\s{0,}')\n",
    "header_sic = re.compile(r'STANDARD INDUSTRIAL CLASSIFICATION:\\s{1,}.*([0-9]{4})\\]{0,}\\s{0,}')\n",
    "\n",
    "# Define input and output directories\n",
    "INPUT_DIR = \"naya/naya2/naya3/SEC2\"\n",
    "OUTPUT_DIR = \"naya/naya2/naya3/SEC2\"\n",
    "\n",
    "# Function to trim whitespace from both ends of a string\n",
    "def trim(s):\n",
    "    return s.strip()\n",
    "\n",
    "# Function to process a text file\n",
    "def process_file(file_path):\n",
    "    print(\"Processing file:\", file_path)\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Process each line\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = line.strip()  # Remove leading and trailing whitespace\n",
    "        # Additional processing if needed\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    # Write cleaned lines to a new file\n",
    "    output_file_path = os.path.join(OUTPUT_DIR, os.path.basename(file_path))\n",
    "    with open(output_file_path, 'w',encoding='utf-8') as output_file:\n",
    "        output_file.write('\\n'.join(cleaned_lines))\n",
    "\n",
    "    print(\"File processed and saved to:\", output_file_path)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # Iterate over files in the input directory\n",
    "    for filename in os.listdir(INPUT_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(INPUT_DIR, filename)\n",
    "            process_file(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39475da",
   "metadata": {},
   "source": [
    "# Description: This script gets the sentences from the footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67705d7",
   "metadata": {},
   "source": [
    "This code removes all the extra spaces and arranges the contents of the files in the form of sentences in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99512e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Change these working directories and files as needed\n",
    "OUTPUTDIR = \"naya/naya2/naya3/SEC2\"\n",
    "\n",
    "# Regular Expressions\n",
    "page = r'(^\\s*?\\n)*(^\\s*?(page\\s*?)?-?\\d{1,3}-?\\s*?\\n)(^\\s*?\\n)*'\n",
    "pageof = r'(^\\s*?\\n)*(^\\s*?(page\\s*?)?\\s*?\\d+\\s*?of\\s*?\\d+?\\s*?\\n)(^\\s*?\\n)*'\n",
    "toc = r'(^\\s*?\\n)*(^\\s*?\\[\\d+\\]\\s*Table\\sof\\sContents\\s*\\n)(^\\s*?\\n)*'\n",
    "fpage = r'(^\\s*?\\n)*(^\\s*?F-\\d+\\s*\\n)(^\\s*?\\n)*'\n",
    "continued = r'(^\\s*?\\n)*(^\\s*?.{0,60}?\\((continued|cont.|cont)\\)\\s*\\n)(^\\s*?\\n)*'\n",
    "\n",
    "# Process all files in the directory\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\"sentences.txt\"):\n",
    "        print(f\"Processing file: {file}\")\n",
    "        sentences_file = file.replace(\"10KNotes\", \"10KNotesSentences\")\n",
    "        output_file_path = os.path.join(OUTPUTDIR, sentences_file)\n",
    "\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"File already exists: {output_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
    "            data = input_file.read()\n",
    "            # Apply regular expressions\n",
    "            data = re.sub(page, '', data)\n",
    "            data = re.sub(pageof, '', data)\n",
    "            data = re.sub(toc, '', data)\n",
    "            data = re.sub(fpage, '', data)\n",
    "            data = re.sub(continued, '', data)\n",
    "\n",
    "            # Sentence segmentation\n",
    "            sentences = sent_tokenize(data)\n",
    "            for sentence in sentences:\n",
    "                output_file.write(sentence.strip() + '\\n')\n",
    "\n",
    "        print(f\"Processed file saved at: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5260a",
   "metadata": {},
   "source": [
    "# Description: This file contains the regular expressions to find the footnote sections, the lists are in order of priority\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306db16",
   "metadata": {},
   "source": [
    "Here, the code can be modified to find the words and the sentence where the word occurs. One can simply copy paste any 1 of the categories and change the word(example: AI) to the desired word(example: appear, communications,etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc69cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define categories and their corresponding regular expressions\n",
    "categories = {\n",
    "    \"AI\": [\n",
    "        re.compile(r'AI(?:s)?', re.IGNORECASE)\n",
    "    ],\n",
    "    \"subs\": [\n",
    "        re.compile(r'subsidiaries(?:s)?', re.IGNORECASE)\n",
    "    ],\n",
    "    \"MICRO\": [\n",
    "        re.compile(r'Microsoft(?:s)?', re.IGNORECASE)\n",
    "    ],\n",
    "    # Add more categories and their regular expressions here...\n",
    "}\n",
    "\n",
    "def match_categories(line):\n",
    "    matches = []\n",
    "    for category, regex_list in categories.items():\n",
    "        for regex in regex_list:\n",
    "            if regex.search(line):\n",
    "                matches.append(category)\n",
    "                break\n",
    "    return matches\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    with open(input_file, 'r') as f_input, open(output_file, 'w') as f_output:\n",
    "        for line in f_input:\n",
    "            matched_categories = match_categories(line)\n",
    "            if matched_categories:\n",
    "                f_output.write(f\"{line.strip()} - Matches: {', '.join(matched_categories)}\\n\")\n",
    "\n",
    "def main():\n",
    "    input_file = 'naya/naya2/naya3/SEC2/target_sentences.txt'\n",
    "    output_file = 'naya/naya2/naya3/SEC2/target1.txt'\n",
    "    process_file(input_file, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e0439",
   "metadata": {},
   "source": [
    "# Description: This script counts the number of estimation words in the notes to the financial statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b410a70",
   "metadata": {},
   "source": [
    "In case, one wants to know the number of occurences of a word, this code will provide the count.\n",
    "\n",
    "In the code below,\n",
    "COR[P|PORATION|PORATIONS]: This part shows that words with CORP or CORPORATION or CORPORATIONS are to be counted. Similarly other words can be written and searched.\n",
    "\n",
    "Example: appear[s|ing|ed]: this tells the code to search for words containing appear or appearing or appeared and to tell the count of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22442f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def trim(string):\n",
    "    return string.strip()\n",
    "\n",
    "# Working directory\n",
    "input_directory = \"C:\\\\Users\\\\Pravalika Das\\\\DATA_WALA\\\\UPDATED_NLP_COURSE\\\\SEC EDGAR\\\\naya\\\\naya2\\\\naya3\\\\SEC2\\\\SEC3\\\\SEC4\\\\SEC5\"\n",
    "# Output file\n",
    "output_file = open(\"C:\\\\Users\\\\Pravalika Das\\\\DATA_WALA\\\\UPDATED_NLP_COURSE\\\\SEC EDGAR\\\\naya\\\\naya2\\\\naya3\\\\SEC2\\\\SEC3\\\\SEC4\\\\untitled_format1.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "# Dictionary to store word counts\n",
    "word_counts = {}\n",
    "\n",
    "# Loop through files in directory\n",
    "for file_name in os.listdir(input_directory):\n",
    "    if file_name.endswith(\".txt\") or file_name.endswith(\".TXT\"):\n",
    "        print(\"Processing file...\", file_name)\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding='utf-8') as temp:\n",
    "                for line in temp:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        # Check if the word is similar to \"appear\" using regex\n",
    "                        if re.match(r'COR[P|PORATION|PORATIONS]*', word, re.IGNORECASE):\n",
    "                            word = word.lower()  # Convert to lowercase for consistency\n",
    "                            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        except FileNotFoundError:\n",
    "            print(\"File doesn't exist... moving on\")\n",
    "            continue\n",
    "\n",
    "# Output word counts to the file\n",
    "for word, count in word_counts.items():\n",
    "    output_file.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "output_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91c0b6",
   "metadata": {},
   "source": [
    "# Description: Parser code similar to the one given in JAVA Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25c018",
   "metadata": {},
   "source": [
    "The following code shows the Parsed file of the actual file. This code was written with reference to the JAVA Code that was provided in the Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ce08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to parse file and store results in another file\n",
    "def parse_file(file_path, output_file):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file, open(output_file, 'a', encoding='utf-8') as output:\n",
    "        # Skip header line\n",
    "        next(file)\n",
    "        \n",
    "        # Process each line\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) <= 425:  # Length check to eliminate outlier formatting errors\n",
    "                output.write(line + '\\n')\n",
    "                doc = nlp(line)\n",
    "                # Write dependencies to output file\n",
    "                for token in doc:\n",
    "                    output.write(f\"{token.text} {token.dep_} {token.head.text} {token.head.pos_} \"\n",
    "                                 f\"{[child for child in token.children]}\\n\")\n",
    "\n",
    "# Input and output directories\n",
    "input_directory = \"C:\\\\Users\\\\Pravalika Das\\\\DATA_WALA\\\\UPDATED_NLP_COURSE\\\\SEC EDGAR\\\\naya\\\\naya2\\\\naya3\\\\SEC2\\\\SEC3\"\n",
    "output_directory = \"C:\\\\Users\\\\Pravalika Das\\\\DATA_WALA\\\\UPDATED_NLP_COURSE\\\\SEC EDGAR\\\\naya\\\\naya2\\\\naya3\\\\SEC2\\\\SEC3\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Iterate over files in input directory\n",
    "import os\n",
    "for file_name in os.listdir(input_directory):\n",
    "    if file_name.endswith(\".txt\") or file_name.endswith(\".TXT\"):\n",
    "        input_file_path = os.path.join(input_directory, file_name)\n",
    "        output_file_path = os.path.join(output_directory, f\"parsed_{file_name}\")\n",
    "        print(\"Processing Input File... \", file_name)\n",
    "        parse_file(input_file_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
